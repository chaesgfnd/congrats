import openai, os, requests, time, json

api_key = os.getenv("OPENAI_KEY")
# second var is cost of 1k tokens
gpt35 = ("gpt-3.5-turbo", 0.002)
gpt4t = ("gpt-4-1106-preview", 0.015)  # price is a guess, and is not to be trusted

api_key = os.getenv("OPENAI_KEY")
openai.api_type = "azure"
openai.api_key = api_key


def generate(context_messages: list[str]) -> str:
	"""
	Generates a cohesive response based on the previous messages in the conversation
	"""

	context_messages = [msg for msg in context_messages if msg is not None]
	messages_string = "\n".join(context_messages)
	q = messages_string
	s = request(question=q, instruction="Wish happy new year based on the previous messages in the conversation")

	return s


def request(question: str, instruction: str, model=gpt35, debug=False):
	system_line = {"role": "system", "content": instruction}
	user_line = {"role": "user", "content": question}
	conversation = [system_line] + [user_line]

	url = "https://api.openai.com/v1/chat/completions"
	headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}

	data = {
		"model": f"{model[0]}",
		"messages": conversation,
		"temperature": 0,
		# "max_tokens": 100,
	}
	start_time = time.time()
	r = requests.post(url, headers=headers, data=json.dumps(data)).json()
	end_time = time.time()

	if debug:
		print(f"{model[0]}:")
		print(f"{conversation[-1]['content']}")
		print(f"{r['choices'][0]['message']['content']}")
		tokens = float(r["usage"]["total_tokens"])
		cost = model[1] * tokens / 1000
		print(f"cost: {cost} in {end_time-start_time}\n")
	return r["choices"][0]["message"]["content"].split(";")[0]


if __name__ == "__main__":
	test_messages = [
		"ĞĞ¿ÑÑ‚ÑŒ Ñ Ğ¿Ğ¾Ğ¿ĞºĞ¾Ñ€Ğ½Ğ¾Ğ¼? ğŸ£",
		"Ğ¯ Ğ²Ğ¾Ñ‚ ÑÑ‚Ğ¾ Ğ¾Ğ´Ğ¾Ğ±Ñ€ÑÑğŸ¤˜",
		"Ñƒ Ğ¼ĞµĞ½Ñ Ğ·Ğ°ĞºĞ¾Ğ½Ñ‡Ğ¸Ğ»ÑÑ ğŸ˜”",
		"ÑƒĞ¶Ğ°Ñ Ğ¼Ğ°Ğ¼Ğ°",
		"Ğ¼Ğ°Ğ¼Ğ° Ğ´Ğ°Ğ¹ Ğ´ĞµĞ½ÑÑˆĞµĞº Ğ½Ğ° ĞºĞ¸Ğ½Ğ¾ Ğ¿Ğ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ° ğŸ™‡ğŸ™‡ğŸ™‡ğŸ™‡",
		"Ğ¢ĞµĞ±Ğµ Ğ½Ğ°Ğ»Ğ¸Ğº Ğ½ÑƒĞ¶ĞµĞ½?",
		"Ğ½Ğµ Ğ·Ğ½Ğ°Ñ",
		"Ğ´Ğ°",
		"ĞŸĞ¾Ğ´Ğ½Ğ¸Ğ¼Ğ¸ÑÑŒ Ğ½Ğ°Ğ²ĞµÑ€Ñ…",
		"ĞšÑƒ-ĞºÑƒ, Ñƒ Ñ‚ĞµĞ±Ñ Ğ²ÑÑ‘ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¾?",
		"Ğ´Ğ° Ñ Ğ¸Ğ´Ñƒ Ğ´Ğ¾Ğ¼Ğ¾Ğ¹ ğŸ‘",
		"ĞŸÑ€Ğ¸Ğ´Ñ‘ÑˆÑŒ, Ğ½Ğµ Ñ€Ğ°Ğ·ÑƒĞ²Ğ°Ğ¹ÑÑ, Ğ²Ñ‹Ğ½ĞµÑĞ¸ Ğ¼ÑƒÑĞ¾Ñ€, Ğ¿Ğ»Ğ¸Ğ·",
		"Ğ¼Ğ½Ğµ Ğ² Ñ‚ÑƒĞ°Ğ»ĞµÑ‚ Ğ½Ğ°Ğ´Ğ¾ Ğ¼Ğ°Ğ¼Ğ°",
		"Ğ¼Ğ°Ğ¼Ğ° Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¾ĞµÑÑ‚ÑŒ ĞºÑ€ĞµĞ²ĞµÑ‚ĞºĞ¸?",
		"Ğ¼Ğ¾Ñ Ñ€Ğ¾Ğ´Ğ½Ğ°Ñ Ğ¼Ğ°Ğ¼Ğ° Ğ° ÑÑ‚Ğ¾ Ñ‚Ñ‹ Ğ² Ğ²Ğ°Ğ½Ğ½Ğµ? ğŸ™‡ğŸ™‡ğŸ™‡ğŸ™‡ğŸ™‡",
		"Ğ¼Ğ¾Ñ Ñ€Ğ¾Ğ´Ğ½Ğ°Ñ Ğ¼Ğ°Ğ¼Ğ° Ğ° Ñ‚ĞµĞ±Ğµ ĞµÑ‰Ñ‘ Ğ´Ğ¾Ğ»Ğ³Ğ¾ ğŸ™‡ğŸ™‡ğŸ™‡ğŸ™‡ğŸ™‡",
	]
	print(generate(test_messages))
